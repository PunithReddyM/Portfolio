<!DOCTYPE html> <html lang="en"> <head> <meta charset="UTF-8" /> <meta name="viewport" content="width=device-width, initial-scale=1.0"/> <meta name="description" content="Punith Reddy Muthi â€“ Data Engineering Portfolio"> 
  <title>Punith Reddy Muthi â€“ Data Engineering Portfolio</title> 
  <link rel="stylesheet" href="styles.css" /> </head> 
  <body> <header> <h1>Punith Reddy Muthi</h1> 
    <p>Senior Data Engineer | FinTech | Cloud Pipelines | Real-time Data</p> 
  </header> <section> <h2>ğŸ‘‹ About Me</h2> <p> Hi, I'm a passionate data engineer with 4+ years of experience in building cloud-native pipelines, optimizing distributed data systems, and delivering business insights in fast-paced financial environments. I've worked with Fortune 500 companies like Liberty Mutual and high-growth organizations like Axis Direct. This portfolio highlights the real-world projects and tools I've worked with, including Trino, Iceberg, Snowflake, DBT, and more. </p> </section> 
  <section> <h2>ğŸ›  Skills & Technologies</h2> <div class="skills"> <span>Trino</span> <span>Apache Iceberg</span> <span>Redshift</span> <span>Snowflake</span> <span>DBT</span> <span>Apache Spark</span> <span>SQL</span> <span>Python</span> <span>Airflow</span> <span>Terraform</span> <span>Git</span> <span>Azure</span> <span>AWS</span> <span>Databricks</span> <span>Kafka</span> </div> </section> 
  <section> <h2>ğŸ“‚ Featured Projects</h2>

    <div class="project">
      <h3>ğŸ¢ Liberty Mutual â€“ Trino & Iceberg Migration</h3>
      <p>
        Migrated core DBT models from Snowflake and Redshift to Trino (Iceberg) for real-time risk modeling.
        Implemented schema evolution and partitioning strategies to support high-throughput queries on
        cloud-native Iceberg tables. Integrated DBT with Git and CI/CD workflows, while syncing Tableau
        dashboards for stakeholder reporting.
      </p>
    </div>
    
    <div class="project">
      <h3>ğŸ’° Axis Direct â€“ Streaming Finance Pipelines</h3>
      <p>
        Designed data pipelines using PySpark and Snowflake to support real-time trading insights.
        Used Kafka and Airflow to orchestrate streaming data and batch ETL jobs.
        Optimized BigQuery and Redshift jobs to reduce cost and latency by over 30%.
      </p>
    </div>
    
    <div class="project">
      <h3>ğŸ“ College Project â€“ DBT Sandbox with Snowflake</h3>
      <p>
        Built a sandbox DBT environment to model customer and orders data using Jinja and Snowflake.
        Focused on modular transformations, CI/CD workflows using GitHub Actions, and metric layer
        documentation with dbt docs.
      </p>
    </div>
    </section> <footer> <p> Made with â¤ï¸ by Punith Reddy Muthi | <a href="https://www.linkedin.com/in/your-profile" target="_blank">LinkedIn</a> | <a href="mailto:your.email@example.com">Email Me</a> </p> </footer> </body> 
    
    </html>